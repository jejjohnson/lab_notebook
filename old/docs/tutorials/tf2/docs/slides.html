<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>reveal-md</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/serif.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />

    <script>
      document.write('<script src="http://' + (location.host || 'localhost').split(':')[0] + ':35729/livereload.js?snipver=1"></' + 'script>');
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><!-- .slide: data-transition="slide" -->
# TF2.X and PyTorch

**For not so Dummies**

J. Emmanuel Johnson
</script></section><section ><section data-markdown><script type="text/template"><!-- .slide: data-transition="slide" -->
## What is Deep Learning?
</script></section><section data-markdown><script type="text/template">

> Deep Learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods. - Yann LeCun


</script></section><section data-markdown><script type="text/template">
> Deep Learning is a collection of tools to build complex modular differentiable functions. - Danilo Rezende
> 
</script></section><section data-markdown><script type="text/template">
#### It's more or less a tool...

* Tensor structures <!-- .element: class="fragment" data-fragment-index="1" -->
* Automatic differentiation (AutoGrad) <!-- .element: class="fragment" data-fragment-index="2" -->
* Model Framework (Layers, etc) <!-- .element: class="fragment" data-fragment-index="3" -->
* Optimizers <!-- .element: class="fragment" data-fragment-index="4" -->
* Loss Functions <!-- .element: class="fragment" data-fragment-index="5" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Software Perspective

* Who is your audience? <!-- .element: class="fragment" data-fragment-index="1" -->
* What's your scope? <!-- .element: class="fragment" data-fragment-index="2" -->
* Modular design <!-- .element: class="fragment" data-fragment-index="3" -->
* Influencing minds... <!-- .element: class="fragment" data-fragment-index="4" -->
</script></section><section data-markdown><script type="text/template">
<p align="center">

  <img src="https://keras-dev.s3.amazonaws.com/tutorials-img/spectrum-of-workflows.png" alt="drawing" width="800"/>
</p>
</script></section><section data-markdown><script type="text/template">
#### User 1

My employer gave me some data of landmass in Africa and wants me to find some huts. He thinks Deep Learning can help.
</script></section><section data-markdown><script type="text/template">
#### User 2

I think I would like one network for my $X$ and $y$. I also think maybe I should have another network with shared weights and a latent space. Maybe I coud also have two or three input locations. In addition...
</script></section><section data-markdown><script type="text/template">
#### User 3

I want to implement a Neural Network with convolutional layers and a noise contrastive prior. The weights of the network will be parameterized by Normal distributions. I would also like a training scheme with a mixture of Importance sampling and variational inference with a custom KLD loss.
</script></section><section data-markdown><script type="text/template">
> One Deep Learning library to rule them all...!

Probably a bad idea...
</script></section></section><section ><section data-markdown><script type="text/template">
#### Deep Learning Library Gold Rush

* Currently more than 10+ mainstream libraries
* All tech companies want a piece
</script></section><section data-markdown><script type="text/template">## Growth of PyTorch


<p align="center">
  <img src="https://thegradient.pub/content/images/2019/10/ratio_medium-1.png" alt="drawing" width="800"/>
</p>
</script></section><section data-markdown><script type="text/template">#### Why?

* Simple (Pythonic) <!-- .element: class="fragment" data-fragment-index="1" -->
* Great API <!-- .element: class="fragment" data-fragment-index="2" -->
* Performance vs Productivity Tradeoff <!-- .element: class="fragment" data-fragment-index="3" -->
* Easy to Install... <!-- .element: class="fragment" data-fragment-index="4" -->

</script></section><section data-markdown><script type="text/template">#### Game: Which Library?

<p align="center">
  <img src="https://pbs.twimg.com/media/DppB0xJUUAAjGi-?format=jpg&name=4096x4096" alt="drawing" width="800"/>
</p>

</script></section><section data-markdown><script type="text/template">#### My Suggestions

* Productivity: **Fastai** <!-- .element: class="fragment" data-fragment-index="1" -->
* From Scratch: **JAX** <!-- .element: class="fragment" data-fragment-index="2" -->
* Research: **PyTorch** <!-- .element: class="fragment" data-fragment-index="3" -->
* Production/Industry: **TensorFlow** <!-- .element: class="fragment" data-fragment-index="4" -->
</script></section></section><section  data-markdown><script type="text/template">## Basics

* Tensors
* Variables
* Automatic differentiation (AutoGrad)

</script></section><section ><section data-markdown><script type="text/template">
## Tensors

<p align="center">
  <img src="https://cdn-images-1.medium.com/freeze/max/1000/1*XIOuiEjfXAXOFa0-w2_pTw.jpeg?q=20" alt="drawing" width="800"/>
</p>
</script></section><section data-markdown><script type="text/template">
#### Constants

```python
# create constant
x = tf.constant([[5, 2], [1, 3]])
print(x)
```

```
tf.Tensor(
[[5 2]
 [1 3]], shape=(2, 2), dtype=int32)
```</script></section><section data-markdown><script type="text/template">
#### Standard

```python
# create ones tensor
t_ones = tf.ones(shape=(2, 1))

# create zeros tensor
t_zeros = tf.zeros(shape=(2, 1))
```</script></section><section data-markdown><script type="text/template">
#### Standard Randomized

```python
# pretty standard
tf.random.normal(shape=(2, 2), mean=0., stddev=1.)

# pretty much the same
tf.random.uniform(shape=(2, 2), minval=0, maxval=10)
```
</script></section></section><section ><section data-markdown><script type="text/template">
## Variables

```python
# set initial value
initial_value = tf.random.normal(shape=(2, 2))

# set variable
a = tf.Variable(initial_value)
```

* Options (constraint, trainable, shape)
* All math operations
</script></section><section data-markdown><script type="text/template">
#### Updates

```python
# new value
b = tf.random.uniform(shape=(2, 2))

# set value
a.assign(b)

# increment (a + b)
a.assign_add(b)

# dencrement (a - b)
a.assign_sub(new_a)
```

</script></section></section><section ><section data-markdown><script type="text/template">
## Gradients

</script></section><section data-markdown><script type="text/template">
#### Gradient Function

```python
# init variable
a = tf.Variable(init_value)
# do operation
c = tf.sqrt(tf.square(a) + tf.square(b))
# calculate gradient ( dc/da )
dc_da = tf.gradients(c, a)
# calculate multiple gradients
dc_da, dc_db = tf.gradients(c, [a, b])
```
</script></section><section data-markdown><script type="text/template">
* **New**: `GradientTape`
* Defines the scope
* literally "record operations"
</script></section><section data-markdown><script type="text/template">
```python
# init variable
a = tf.Variable(init_value)

# define gradient scope
with tf.GradientTape() as tape:
    # do operation
    c = tf.sqrt(tf.square(a) + tf.square(b))
    # extract gradients ( dc/da )
    dc_da = tape.gradient(c, a)
```
</script></section><section data-markdown><script type="text/template">
#### Nested Gradients


```python
# init variable
a = tf.Variable(init_value)

# define gradient scope
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        # do operation
        c = tf.sqrt(tf.square(a) + tf.square(b))
        # extract gradients ( dc/da )
        dc_da = tape.gradient(c, a)
    # extract gradients ( d2c/da2 )
    d2c_da2 = outer_tape.gradient(dc_da, a)
```
</script></section></section><section ><section data-markdown><script type="text/template">
### Gradients in PyTorch

* Same gradient function `torch.autograd.grad`
* There is no `Tape`
* Each variable has their own gradient
</script></section><section data-markdown><script type="text/template">
```python
# init variable
a = torch.tensor(init_value, requires_grad=True)
# do operation
c = math.sqrt(a ** 2 + b ** 2)
# calculate gradients ( dc/da )
c.backward(a)
# extract gradients
dc_da = a.grad
```
</script></section></section><section ><section data-markdown><script type="text/template">
## TF: Engine Module

* `Layer`
* `Network` - DAG graph
* `Model`
* `Sequential` 
</script></section><section data-markdown><script type="text/template">
#### Various Subclasses

* Layers
* Metric
* Loss
* Callbacks
* Optimizer 
* Regularizers, Constraints
</script></section></section><section ><section data-markdown><script type="text/template">
## `Layer` Class

* The core abstraction
* Everything is a Layer
* ...or interacts with a layer


</script></section><section data-markdown><script type="text/template">
### Example Layer

$$y = \mathbf{W}x + b$$

```python
# Subclass Layer
class Linear(tf.keras.Layer):
    def __init__(self):
        super().__init__()
        # Make Parameters
    
    def call(self, inputs):
        # Do stuff
        return inputs

```

</script></section><section data-markdown><script type="text/template">
#### 1 - Constructor

```python
# Inherit Layer class
class Linear(tf.keras.Layer):
    def __init__(self, units=32, input_dim=32):
        super().__init__()
```
</script></section><section data-markdown><script type="text/template">
#### 2 - Parameters, $\mathbf{W}$

```python
        # initialize weights (random)
        w_init = tf.random_normal_initializer()(
            shape=(input_dim, units)
        )
        # weights parameter
        self.w = tf.Variable(
            initial_value=w_init,
            trainable=True
       	)
```
</script></section><section data-markdown><script type="text/template">
#### 2 - Parameter, $b$

```python
        # initialize bias (zero)
        b_init = tf.zeros_initializer()(
            shape=(units,)
        )
        # bias parameter
        self.b = tf.Variable(
        	initial_value=b_init,
        	trainable=True
        )
```

</script></section><section data-markdown><script type="text/template">
#### 3 -  Call Function, $\mathbf{W}x +b$

```python
    def call(self, inputs):
        return tf.matmul(inputs, self.w) + b
```
</script></section><section data-markdown><script type="text/template">
```python
class Linear(tf.keras.Layer):
    def __init__(self, units=32, input_dim=32):
        super().__init__()
        w_init = tf.random_normal_initializer()(
            shape=(input_dim, units)
        )
        # weights parameter
        self.w = tf.Variable(
            initial_value=w_init,
            trainable=True
       	)
        # initialize bias (zero)
        b_init = tf.zeros_initializer()(
            shape=(units,)
        )
        # bias parameter
        self.b = tf.Variable(
        	initial_value=b_init,
        	trainable=True
        )
    def call(self, inputs):
        return tf.matmul(inputs, self.w) + b
```
</script></section><section data-markdown><script type="text/template">#### PyTorch (the same...)

```python
class Linear(nn.Module):
    def __init__(self, units: int, input_dim: int):
        super().__init__()
        # weight 'matrix'
        self.weights = nn.Parameter(
            torch.randn(input_dim, units) / math.sqrt(input_dim),
            requires_grad=True
        )
        # bias vector
        self.bias = nn.Parameter(
            torch.zeros(units),
            requires_grad=True
        )

    def forward(self, inputs):
        return inputs @ self.weights + self.bias
```
</script></section><section data-markdown><script type="text/template">
## Using it

```python
# data
x_train = ...

# initialize linear layer
linear_layer = Linear(units=4, input_dim=2)

# same thing as linear_layer.call(x)
y = linear_layer(x)
```
</script></section></section><section ><section data-markdown><script type="text/template">
#### TensorFlow `build`

* Know the # of nodes 
* Don't know the input shape
* More conventional
</script></section><section data-markdown><script type="text/template">
For example...

```python
    def build(self, input_shape):
        # Weights variable
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units), 
            initializer='random_normal',
            trainable=True
        )
        # Bias variable
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )
```
</script></section><section data-markdown><script type="text/template">
More convenient...
```python
# data
x_train = ...

# initialize linear layer (without input dims)
linear_layer = Linear(units=4)

# internally -> calls x.shape
y = linear_layer(x)
```
</script></section></section><section ><section data-markdown><script type="text/template">
We can nest as many `Layers` as we want.
</script></section><section data-markdown><script type="text/template">
#### Linear

```python
class Linear(Layer):
    def __init__(self, units=32):
        super().__init__()
        # call linear layer
        self.linear = Linear(units)

    def call(self, inputs):
        x = self.linear(inputs)
        return x
```
</script></section><section data-markdown><script type="text/template">
#### Linear Block 

```python
class LinearBlock(Layer):
    def __init__(self):
        super().__init__()
        self.lin_1 = Linear(32)
        self.lin_2 = Linear(32)
        self.lin_3 = Linear(1)
    
    def call(self, inputs):
        x = self.lin_1(x)
        x = self.lin_2(x)
        x = self.lin_3(x)
        return x
```
</script></section></section><section ><section data-markdown><script type="text/template">
## Training TF2.X, PyTorch
</script></section><section data-markdown><script type="text/template">#### Losses

TensorFlow
```python
# example loss function 
loss_func = torch.nn.MSELoss()
```
PyTorch
```python
# example loss function
loss_fn = tf.keras.losses.MSELoss()
```
</script></section><section data-markdown><script type="text/template">#### Optimizers

TensorFlow

```python
# example optimizer
optimizer = tf.keras.optimizers.Adam()
```
PyTorch
```python
# example optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)
```
</script></section><section data-markdown><script type="text/template">
#### Full Training Loop (PyTorch)

```python
# Loop through batches
for x, y in dataset:
    # initialize gradients
    optimizer.zero_grad()
    # predictions for minibatch
    ypred = lr_model(xbatch)
    # loss value for minibatch
    loss = loss_func(ypred, ybatch)
    # find gradients
    loss.backward()
    # apply optimization
    optimizer.step()
```
</script></section><section data-markdown><script type="text/template">
#### Full Training Loop (TF2.X)

```python
for x, y in dataset:
    with tf.GradientTape() as tape:
        # predictions for minibatch
        preds = model(x)
        # loss value for minibatch
        loss = loss_fn(y, preds)
    # find gradients
    grads = tape.gradients(loss, model.trainable_weights)
    # apply optimization
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
```
</script></section></section><section ><section data-markdown><script type="text/template">
## TensorFlow Nuggets
</script></section><section data-markdown><script type="text/template">## Training Call

* Allows training versus inference mode
* Just need an extra argument `training=True` in the `call` method
* Prob Models, e.g. Batch Norm., Variational Inference
</script></section><section data-markdown><script type="text/template">#### Example

```python
...
def call(self, x, training=True):
    if training:
        # do training stuff
    else:
        # do inference stuff
    return x
```
</script></section><section data-markdown><script type="text/template">### Add Loss

* "Add Losses on the fly"
* Each layer has it's own regularization
* Examples: KLD, Activation or Weight Regularization
</script></section><section data-markdown><script type="text/template">#### Example - Model

```python
class MLP(Layer):
    def __init__(self, units=32, reg=1e-3):
        super().__init__()
        self.linear = Linear(units)
        self.reg = reg
    def call(self, inputs):
        x = self.linear(inputs)
        x = tf.nn.relu(x)
        # Add loss during the call
        self.add_loss(tf.reduce_sum(output ** 2) * self.reg)
        return x
```</script></section><section data-markdown><script type="text/template">#### Example - Training


```python
mlp_model = MLP(32)                 # initialize model
loss_fn = tf.keras.losses.MSELoss() # loss function
opt = tf.keras.optimizers.Adam()    # optimizer
# Loop through dataset
for x, y in dataset:
    with tf.GradientTape() as tape:
        preds = mlp_model(x)            # predictions
        loss = loss_fn(y, preds) 		# loss value
        loss += sum(mlp_model.losses)	# extra losses
    # find gradients
    grads = tape.gradients(loss, model.trainable_weights)
    # apply optimization
    opt.apply_gradients(zip(grads, model.trainable_weights))
```
</script></section><section data-markdown><script type="text/template">### Compile Code

* Use a decorator, `@tf.function`
* Optional
* Easy performance booster
</script></section><section data-markdown><script type="text/template">#### Example - Graphs

```python
@tf.function
def train_step(dataset):
    for x, y in dataset:
        with tf.GradientTape() as tape:
            preds = mlp_model(x)            # predictions
            loss = loss_fn(y, preds) 		# loss value
            loss += sum(mlp_model.losses)	# extra losses
        # find gradients
        grads = tape.gradients(loss, model.trainable_weights)
        # apply optimization
        opt.apply_gradients(zip(grads, model.trainable_weights))
        return loss
```

</script></section></section><section ><section data-markdown><script type="text/template">## Model Class

* Can do everything a `Layer` can do
* Built-in functionality
* a.k.a. Keras territory
* TF and PyTorch part ways
</script></section><section data-markdown><script type="text/template">#### Definitions


**Layer**: 

* A closed sequence of operation
*  e.g. convolutional layer, recurrent layer, resnet block, attention block.

**Model**: 

* The top layer of your algorithm
* e.g. Deep learning model, deep neural network.
</script></section><section data-markdown><script type="text/template">#### Training Functionality 

* `.compile()`
* `.fit()`
* `.evaulate()`
* `.predict()`
* `.save()`
* `.summary()`
* `.plot_model()`
</script></section><section data-markdown><script type="text/template">#### Example - 

```python
# loss function
loss = tf.keras.losses.MSELoss(from_logits=True)
# accuracy metrics
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
# optimizer
optimizer = tf.keras.optimizers.Adam()
# compile to graph
model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])
# Fit Model
model.fit(dataset, epochs=3)
# Test Data
loss, acc = model.evaluate(test_dataset)
```
</script></section></section><section ><section data-markdown><script type="text/template">## Functional Models


* Creates DAG <!-- .element: class="fragment" data-fragment-index="1" -->
* Model Class with Extras <!-- .element: class="fragment" data-fragment-index="2" -->
* Only in TF <!-- .element: class="fragment" data-fragment-index="3" -->
</script></section><section data-markdown><script type="text/template">
<p align="center">
  <img src="https://d3i71xaburhd42.cloudfront.net/896aa86d61a5dc506ee44fb5527988100a12e761/2-Figure1-1.png" alt="drawing" width="800"/>
</p>
</script></section><section data-markdown><script type="text/template">#### Simple Example 

```python
# input checks
x = tf.keras.layers.Flatten(shape=28, 28))(inputs)
# Layer 1
x = tf.keras.layers.Dense(512, activation=tf.nn.relu)(inputs)
# Layer 2
x = tf.keras.layers.Dropout(0.2)(x)
# outputs
x = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x)
# create model class
model = tf.keras.Model(inputs, outputs)
# compile
model.compile(
    optimizer='adam', 
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```
</script></section><section data-markdown><script type="text/template">#### Example - Graph Output

<p align="center">
  <img src="https://miro.medium.com/max/2036/0*H5K5RhjSiEjZg5oU" alt="drawing" width="500"/>
</p>
</script></section><section data-markdown><script type="text/template">#### We can go crazy...
<p align="center">
  <img src="https://miro.medium.com/max/3200/0*eTqoj1dNmBZhuQvq" alt="drawing" width="900"/>
</p>

</script></section></section><section ><section data-markdown><script type="text/template">
## Sequential Models

* Predifined <!-- .element: class="fragment" data-fragment-index="1" -->
* PyTorch & TF <!-- .element: class="fragment" data-fragment-index="2" -->
* In TF, Model class <!-- .element: class="fragment" data-fragment-index="3" -->
</script></section><section data-markdown><script type="text/template">**PyTorch**

```python
model = nn.Sequential(
  torch.nn.Linear(256),
  F.reLU(),
  torch.nn.Linear(256),
  F.reLU(),
  torch.nn.Linear(10),
)
```

**TensorFlow**

```python
model = tf.keras.Sequential([
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(10)
])
```
</script></section></section><section ><section data-markdown><script type="text/template">
## Datasets

* Convenience Functions
* Take care of loading, iterations, batches
</script></section><section data-markdown><script type="text/template">### Normally

```python
n_batches = (n_samples - 1) // batch_size + 1

for idx in range(n_batches):
    # get indices for batches
    start_idx = idx * batch_size
    end_idx   = start_idx + batch_size
    # get subset from data
    xbatch = x_train[start_idx:end_idx]
    ybatch = y_train[start_idx:end_idx]
```

</script></section><section data-markdown><script type="text/template">### PyTorch - Datasets

```python
# create dataset
train_ds = TensorDataset(x_train, y_train)
# Loop through batches
    for start_idx, end_idx in range(batch_idx):
        # Use Dataset to store training data
        xbatch, ybatch = train_ds[start_idx:end_idx]
        # Do stuff...
```

<aside class="notes"><p>In PyTorch, the <code>Dataset</code> helps us to do index and slice through our data. It also can combine inputs and outputs so that we only have to slice through a single dataset. It can even convert your <code>np.ndarray</code> dataset to a Tensor automatically.</p>
</aside></script></section><section data-markdown><script type="text/template">### PyTorch - DataLoaders

```python
# create dataset
train_ds = TensorDataset(x_train, y_train)
# create dataloader
train_dl = DataLoader(train_ds, batch_size=100)
# Loop through batches
    for xbatch, ybatch in train_dl:
        # Do stuff...
```

</script></section><section data-markdown><script type="text/template">### TF - Both...

```python
# create dataset
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)
)
# create dataloader
train_dl = train_ds.batch(100)
# Loop through batches
    for xbatch, ybatch in train_dl:
        # Do stuff...
```
</script></section></section><section ><section data-markdown><script type="text/template">
## What We Covered

* DL Framework Idea
* Layers and Models
* Sequential Model
</script></section><section data-markdown><script type="text/template">
## What We didn't Cover

* Callbacks
* Distributed Training
* Multiple GPUs
* All options under the sun
* Tensorboard (Built-in Jupyter Notebooks!)
</script></section></section><section ><section data-markdown><script type="text/template">## Summary

<p align="center">
  <img src="https://keras-dev.s3.amazonaws.com/tutorials-img/model-building-spectrum.png" alt="drawing" width="800"/>
</p>
</script></section><section data-markdown><script type="text/template">
## TensorFlow Training

<p align="center">
  <img src="https://keras-dev.s3.amazonaws.com/tutorials-img/model-training-spectrum.png" alt="drawing" width="800"/>
</p></script></section></section></div>
    </div>

    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
